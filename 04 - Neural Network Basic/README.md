# 신경망 구현

## 딥러닝 기본 개념

+ 인공신경망(artificial neural network) : 인간의 뇌인 뉴런을 모방하여 만든 일련의 계산과정
  + 입력(y)을 받아서 가지고 있던 가중치(W), 편향(B), 활성화 함수(sigmoid)에 따라서 출력을 하는 (인공)뉴런들의 다발이다.\
  y = S (X * W + B)
  + 인공 뉴런(artificial neuron) : 인공신경망을 이루는 한 단위, 신경망의 노드이다.
    + 일반적으로 하나 **이상**의 입력에 대해 하나의 출력을 한다.
    + 뉴런은 입력 값에 가중치, 편향과 활성화 함수(비선형 변환)를 적용하여 출력 값을 계산한다.
    + 딥러닝에서 '뉴런'이라고하면 보통은 인공 뉴런이다.
  + 학습(learning) / 훈련(train) : 가중치와 편향의 적절한 값을 찾아가는 과정
  + 활성화 함수(activation function) : 인공 뉴런에서 출력값의 형태에 영향을 끼친다.
    + ex) Sigmoid(시그모이드), ReLU(렐루), Adam(아담) 등
  + 레이어(layer) : 신경망의 뉴런 집합 혹은 입력 특성의 집합을 의미한다.
    + 입력층(input layer) : 신경망의 첫 번째 레이어로서 입력 데이터를 받는다.
    + 출력층(output layer) : 신경망의 '최종' 레이어이다.
  + 레이블(label) : 입력값에 대한 정답을 의미한다.\
  기계가 학습할 때에는 입력값과 레이블을 주고 예측할 때에는 입력만 주고 레이블을 유추하도록 한다.
  + 참고 링크\
  https://sacko.tistory.com/17
    
+ 역전파(backpropagation) : 신경망에서 경사하강법을 수행하는 기본 알고리즘이다.\
학습을 진행할 때 오차값에 대한 더 적절한 가중치나 편향의 값을 찾아내도록 하는 방법이다.

## 역전파

딥러닝의 필수요소이며 계산이 오래걸리는 요인 중 하나로 개념은 오래전부터 존재했지만 컴퓨터 성능의 부족으로 현재에 와서 재조명된 알고리즘이다.

전파는 출력을 계산하기 위한 방법이고 (위의 y = S (X * W + B) 식이 해당됨)\
역전파는 가중치와 편향을 계산하기 위한 방법이다. (식이 어려우므로 생략, 자세한 내용은 구글링하면 많이 나온다.)

둘을 자세히 알기 위해서 미적분의 미분과 적분의 개념만 알아도 이해가 가능하다.\
하지만 구구단은 알지만 1234\*5678을 쉽게 계산하기 어려운 것처럼 계산량이 압도적이라서\
정말 간단한 인공신경망이 아니면 역전파 계산하기가 쉽지 않다;

하지만 한번은 꼭 개념을 잡고 가는 것을 추천하며 알고 있어도 다시 보는 것을 권장한다.

이런 어려운 인공신경망보다 더 대단한 내 뇌는 참 똑똑한 것 같다 :)

## 분류 모델 실습 (단일 신경망)

딥러닝은 다양한 분야에서 쓰이고 다양한 주제의 문제를 해결하는데 사용하고있다.\
그 중에서도 가장 인기가 있는 주제 중 하나가 패턴 인식을 통한 영상 분류 모델이 있다.

실재로 이미지를 넣고 바로 분류를 해보면 좋겠으나 \
일단은 '분류'에만 초점을 두어 이미지대신 간단한 데이터로 분류 모델을 만들어보겠다.

+ 분류(classification) : 둘 이상의 불연속 클래스를 구분하는 것
  + 이진 분류(binary classification) : 분류 중 하나로 두 클래스를 구분한다.
  + 다중 클래스 분류(multi-class classification) : 분류 중 하나로 셋 이상의 클래스를 구분한다.
+ 손실함수(loss function) / 비용함수(cost function) : 예측을 얼마나 정확히 했는지 계산하기 위한 함수
+ 교차 엔트로피(Cross-Entropy) :  다중 클래스 분류 문제에 쓰이는 손실 함수이다.
  + 통계에서 등장하는 용어로 손실 함수 종류 중 하나이다.
  + https://3months.tistory.com/436

---
### 예시 코드

1.15.0 버전 : [01_Classification.ipynb](https://github.com/ii200400/Tensorflow_Tutorial/blob/master/04%20-%20Neural%20Network%20Basic/01_Classification.ipynb)

2.1.0-rc1 버전 : [01_Classification(2_1ver).ipynb](https://github.com/ii200400/Tensorflow_Tutorial/blob/master/04%20-%20Neural%20Network%20Basic/01_Classification(2_1ver).ipynb)

위의 실습코드는 신경망을 한 층으로만 하였기 때문에 정확도가 높지는 않다.\
(학습을 진행할 때마다 변수가 달라지는데 우연히 100% 정확도를 보기도 하였다.)\
다음 실습에서 여러 층의 신경망으로 실습을 진행해보겠다.

---

## 분류 모델 실습 (다중 신경망)

위와 비슷한 실습을 해보겠다. 

신경망의 깊이와 사용하는 최적화 함수 등을 제외하고는 대부분 비슷하다.

+ 은닉층(hidden layer) : 레이어들 중에서 입력층과 출력층을 제외한 중간에 있는 층을 의미한다.

---
### 예시 코드

1.15.0 버전 : [02_Deep_NN.ipynb](https://github.com/ii200400/Tensorflow_Tutorial/blob/master/04%20-%20Neural%20Network%20Basic/02_Deep_NN.ipynb)

2.1.0-rc1 버전 : [02_Deep_NN(2_1ver).ipynb](https://github.com/ii200400/Tensorflow_Tutorial/blob/master/04%20-%20Neural%20Network%20Basic/02_Deep_NN(2_1ver).ipynb)

---
