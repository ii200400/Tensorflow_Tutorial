{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.Autocomplete(2.1ver).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzftUTY3tGq2sfyMOWFgMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ii200400/Tensorflow_Tutorial/blob/master/10%20-%20RNN/02_Autocomplete(2_1ver).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j26GJpGUB-cZ",
        "colab_type": "text"
      },
      "source": [
        "# 개요\n",
        "\n",
        "자연어 처리나 음성 처리 분야에 많이 사용되는 RNN 의 기본적인 사용법을 익히자!\n",
        "\n",
        "4개의 글자를 가진 단어를 학습시켜, 3글자만 주어지면 나머지 한 글자를 추천하여 단어를 완성하는 프로그램을 만들었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjTfnc96h2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "94jrl1LK1UGa"
      },
      "source": [
        "## 데이터 정의\n",
        "\n",
        "평소처럼 이미 있는 데이터 셋을 가져와서 하는 것이 아니라 임의로 데이터를 만들어서 훈련 및 테스트를 한다.\n",
        "\n",
        "때문에 아주 제한적인 상황에서만 쓰일 수 있는 모델이 만들어지며 사용하는 데이터는 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8jD6OqjQBt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
        "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
        "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
        "            'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "# one-hot 인코딩 및 디코딩을 사용 하기 위해 딕셔너리를 만든다.\n",
        "# {'a': 0, 'b': 1, 'c': 2, ..., 'j': 9, 'k', 10, ...}\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "dic_len = len(num_dic)\n",
        "\n",
        "# 다음 배열은 입력값과 출력값으로 아래와 같은 형식을 가진다.\n",
        "# wor -> X, d -> Y\n",
        "# woo -> X, d -> Y\n",
        "seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1FezU7xGd3R",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cmwy9i9OVIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_batch(seq_data):\n",
        "  input_batch = []\n",
        "  target_batch = []\n",
        "\n",
        "  for seq in seq_data:\n",
        "    # 여기서 생성하는 input_batch 와 target_batch 는\n",
        "    # 알파벳 배열의 인덱스 번호이다.\n",
        "    # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
        "    input = [num_dic[n] for n in seq[:-1]]\n",
        "\n",
        "    # 3, 3, 15, 4, 3 ...\n",
        "    target = num_dic[seq[-1]]\n",
        "\n",
        "    # one-hot 인코딩을 한다.\n",
        "    # input 이 [0, 1, 2] 라면\n",
        "    # [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        "    #  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        "    #  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
        "    # numpy-array[리스트] 와 같은 형식은 아래의 셀에 예시를 만들어 놓았다.\n",
        "    input_batch.append(np.eye(dic_len)[input])\n",
        "\n",
        "    # 지금까지 손실함수로 사용하던 softmax_cross_entropy_with_logits 함수는\n",
        "    # label 값을 one-hot 인코딩으로 넘겨줘야 하지만,\n",
        "    # 이 예제에서 사용할 손실 함수인 sparse_softmax_cross_entropy_with_logits 는\n",
        "    # one-hot 인코딩을 사용하지 않으므로 index 로 바로 넘긴다.\n",
        "    target_batch.append(target)\n",
        "\n",
        "  return np.array(input_batch), np.array(target_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbUXvoLMMRid",
        "colab_type": "text"
      },
      "source": [
        "## 모델 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jI80hRik8uS",
        "colab_type": "text"
      },
      "source": [
        "### 옵션 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyor5_5YMRRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "total_epoch = 30\n",
        "\n",
        "# RNN 을 구성하는 시퀀스의 갯수 (문자 내 알파벳 갯수)\n",
        "n_step = 3\n",
        "\n",
        "# 입력값과 출력값의 크기는\n",
        "# 알파벳에 대한 one-hot 인코딩이므로 26개가 된다.\n",
        "# 예) c => [0 0 1 0 0 0 0 0 0 0 0 ... 0]\n",
        "n_input = n_class = dic_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPxNzUJaMUVU",
        "colab_type": "text"
      },
      "source": [
        "### 신경망 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BblgVWShlHyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "27a5b786-8bbd-48ea-af0b-5c7e89ecb649"
      },
      "source": [
        "def make_model():\n",
        "  # 과적합 방지를 위해 Dropout 기법을 사용한 RNN 셀을 생성한다.\n",
        "  cell1 = tf.keras.layers.LSTMCell(n_hidden, dropout=0.5)\n",
        "  # 여러개의 셀을 조합해서 사용하기 위해 셀을 추가로 생성\n",
        "  cell2 = tf.keras.layers.LSTMCell(n_hidden)\n",
        "  # 위에서 만든 셀들을 조합한 RNN 셀을 생성한다.\n",
        "  multi_cell = tf.keras.layers.StackedRNNCells([cell1, cell2])\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    # 함수를 이용해 순환 신경망을 만든다.\n",
        "    tf.keras.layers.RNN(multi_cell, dtype=tf.float32, input_shape=(n_step, n_input)),\n",
        "    tf.keras.layers.Dense(n_class, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = make_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rnn (RNN)                    (None, 128)               210944    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 26)                3354      \n",
            "=================================================================\n",
            "Total params: 214,298\n",
            "Trainable params: 214,298\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rBGzFMR88mv",
        "colab_type": "text"
      },
      "source": [
        "### 비용 및 최적화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of_w2oMV9AA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc0YpcQnCKti",
        "colab_type": "text"
      },
      "source": [
        "### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUcoWDs3CJmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a2e9890-1e9c-499e-bba2-6b2c1ab852f0"
      },
      "source": [
        "input_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "model.fit(x=input_batch,\n",
        "          y=target_batch,\n",
        "          epochs=total_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2626 - accuracy: 0.1000\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.1631 - accuracy: 0.6000\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.9328 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.3951 - accuracy: 0.6000\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9491 - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.7874 - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.7364 - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1798 - accuracy: 0.6000\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3351 - accuracy: 0.3000\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2715 - accuracy: 0.4000\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0719 - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1920 - accuracy: 0.6000\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8460 - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.9258 - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8152 - accuracy: 0.6000\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.9245 - accuracy: 0.6000\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.7000\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6597 - accuracy: 0.7000\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.5561 - accuracy: 0.8000\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6185 - accuracy: 0.6000\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.8460 - accuracy: 0.6000\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4768 - accuracy: 0.8000\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.9857 - accuracy: 0.7000\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.6000\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.9401 - accuracy: 0.7000\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7748 - accuracy: 0.7000\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4120 - accuracy: 0.8000\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3878 - accuracy: 0.8000\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3045 - accuracy: 0.9000\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9314 - accuracy: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8216e42668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7JiFeromLrO",
        "colab_type": "text"
      },
      "source": [
        "### 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qje-t9PomN0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c476e609-8dfb-4562-a5f8-da48efdd01de"
      },
      "source": [
        "# 레이블값이 정수이므로 예측값도 정수로 출력하도록 한다.\n",
        "predict = tf.argmax(model(input_batch), 1)\n",
        "# 정수인 입력값을 비교한다.\n",
        "prediction_check = tf.equal(prediction, target_batch)\n",
        "accuracy_val = tf.reduce_mean(tf.cast(prediction_check, tf.float32)).numpy()\n",
        "\n",
        "predict_words = []\n",
        "for idx, val in enumerate(seq_data):\n",
        "  last_char = char_arr[predict[idx]]\n",
        "  predict_words.append(val[:3] + last_char)\n",
        "\n",
        "print('\\n=== 예측 결과 ===')\n",
        "print('입력값:', [w[:3] + ' ' for w in seq_data])\n",
        "print('예측값:', predict_words)\n",
        "print('정확도:', accuracy_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=== 예측 결과 ===\n",
            "입력값: ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
            "예측값: ['word', 'wood', 'deep', 'dive', 'cold', 'cood', 'load', 'love', 'kiss', 'kind']\n",
            "정확도: 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hggZKP2WuI5c",
        "colab_type": "text"
      },
      "source": [
        "## 전체 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6X9oOp-uLJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#########\n",
        "# 데이터 정의\n",
        "######\n",
        "\n",
        "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
        "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
        "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
        "            'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "dic_len = len(num_dic)\n",
        "\n",
        "seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
        "\n",
        "def make_batch(seq_data):\n",
        "  input_batch = []\n",
        "  target_batch = []\n",
        "\n",
        "  for seq in seq_data:\n",
        "    input = [num_dic[n] for n in seq[:-1]]\n",
        "    target = num_dic[seq[-1]]\n",
        "\n",
        "    input_batch.append(np.eye(dic_len)[input])\n",
        "    target_batch.append(target)\n",
        "\n",
        "  return np.array(input_batch), np.array(target_batch)\n",
        "\n",
        "#########\n",
        "# 옵션 설정\n",
        "######\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "total_epoch = 30\n",
        "\n",
        "n_step = 3\n",
        "\n",
        "n_input = n_class = dic_len\n",
        "\n",
        "#########\n",
        "# 신경망 모델 구성\n",
        "######\n",
        "\n",
        "def make_model():\n",
        "  cell1 = tf.keras.layers.LSTMCell(n_hidden, dropout=0.5)\n",
        "  cell2 = tf.keras.layers.LSTMCell(n_hidden)\n",
        "  multi_cell = tf.keras.layers.StackedRNNCells([cell1, cell2])\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.RNN(multi_cell, dtype=tf.float32, input_shape=(n_step, n_input)),\n",
        "    tf.keras.layers.Dense(n_class, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = make_model()\n",
        "# model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#########\n",
        "# 신경망 모델 학습\n",
        "######\n",
        "\n",
        "input_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "model.fit(x=input_batch,\n",
        "          y=target_batch,\n",
        "          epochs=total_epoch)\n",
        "\n",
        "#########\n",
        "# 결과 확인\n",
        "######\n",
        "\n",
        "predict = tf.argmax(model(input_batch), 1)\n",
        "prediction_check = tf.equal(prediction, target_batch)\n",
        "accuracy_val = tf.reduce_mean(tf.cast(prediction_check, tf.float32)).numpy()\n",
        "\n",
        "predict_words = []\n",
        "for idx, val in enumerate(seq_data):\n",
        "  last_char = char_arr[predict[idx]]\n",
        "  predict_words.append(val[:3] + last_char)\n",
        "\n",
        "print('\\n=== 예측 결과 ===')\n",
        "print('입력값:', [w[:3] + ' ' for w in seq_data])\n",
        "print('예측값:', predict_words)\n",
        "print('정확도:', accuracy_val)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}